{"cells":[{"metadata":{},"cell_type":"markdown","source":["Project overview:\n","\n","- Dataset used: https://www.kaggle.com/gobsan/st-george-or-not\n","- Dataset description: binary classification of ~6000 pics with St. George.\n","\n","This NB attempts to classify a given dataset with Saint George using Pytorch and a pretrained resnet50 model. \n","No extra images were added to the dataset.\n","\n","This is what was done:\n","\n","1. Prep work included using Pandas for path and label collection of all data.\n","2. The data was split into train, valid and test datasets.\n","3. Some minor augmentations added to the data.\n","4. The model was 1st trained with all but the last layer frozen for a couple of epochs, then all params were unfrozen and the whole model trained again.\n","5. tta was used on test dataset.\n","\n","The model shows signs of overfitting the train set, but valid and, most importantly, test set error rates are good indicators of the model working properly."]},{"metadata":{},"cell_type":"markdown","source":["# data preparation"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# this is needed for tta at the end\n","# !pip install ttach"],"execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# imports\n","import torch\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","import time\n","from PIL import Image\n","import ttach as tta\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","\n","import albumentations as A\n","import albumentations.pytorch\n","from sklearn.model_selection import train_test_split"],"execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# setting seed\n","torch.manual_seed(0)\n","np.random.seed(0)"],"execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# setting device to cuda if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# HYPERPARAMS HERE\n","\n","# params\n","lr = 3e-3\n","momentum = 0.9\n","weight_decay = 3e-3\n","\n","# transforms\n","presize = 256\n","crop = 256\n","\n","# batch size\n","batch_size = 32\n","\n","# n_epochs\n","frozen = 1\n","unfrozen = 1"],"execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# data preparation\n","path = 'data' # for local use\n","path_g = os.path.join(path, 'resized_smallest_g')\n","path_ng = os.path.join(path, 'resized_smallest_ng')\n","\n","data_g = os.listdir(path_g)\n","data_ng = os.listdir(path_ng)\n","\n","df_g = pd.DataFrame({'fnames': [os.path.join(path_g, i) for i in data_g], 'label':1})\n","df_ng = pd.DataFrame({'fnames': [os.path.join(path_ng, i) for i in data_ng], 'label':0})\n","\n","df = pd.concat([df_g, df_ng], ignore_index=True)\n","\n","print('df shape before re', df.shape)\n","df = df[df.fnames.str.contains('.*\\.jpg$')]\n","print('df shape after re', df.shape)\n","\n","df = df.sample(frac=1).reset_index(drop=True)\n","\n","# df.head(10)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["df shape before re (504, 2)\ndf shape after re (504, 2)\n"]}]},{"metadata":{},"cell_type":"markdown","source":["# train_test_split"]},{"metadata":{"trusted":true},"cell_type":"code","source":["train_df = df.iloc[:380, :].reset_index(drop=True)\n","valid_df = df.iloc[380:440, :].reset_index(drop=True)\n","test_df = df.iloc[440:, :].reset_index(drop=True)\n","\n","train_df.shape, valid_df.shape, test_df.shape"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((380, 2), (60, 2), (64, 2))"]},"metadata":{},"execution_count":7}]},{"metadata":{},"cell_type":"markdown","source":["# transforms"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# albumentations transforms\n","\n","train_transforms = A.Compose([\n","    A.SmallestMaxSize(presize),\n","    A.RandomCrop(crop, crop),\n","    A.Normalize(),\n","    A.HorizontalFlip(),\n","    A.Rotate(limit=30),\n","    albumentations.pytorch.ToTensorV2()\n","    ])\n","\n","valid_transforms = A.Compose([\n","    A.SmallestMaxSize(presize),\n","    A.CenterCrop(crop, crop),\n","    A.Normalize(),\n","    A.HorizontalFlip(),\n","    albumentations.pytorch.ToTensorV2()\n","    ])\n","\n","test_transforms = A.Compose([\n","    A.SmallestMaxSize(presize),\n","    A.CenterCrop(crop, crop),\n","    A.Normalize(),\n","    albumentations.pytorch.ToTensorV2()\n","    ])"],"execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# defining dataset class"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class dataset(Dataset):\n","    def __init__(self, df, transforms=None):\n","        self.df=df\n","        self.transforms=transforms\n","        \n","    def __len__(self):\n","        return self.df.shape[0]\n","    \n","    def __getitem__(self, index):\n","        image = Image.open(self.df.fnames[index]).convert('RGB')\n","        image = np.array(image)\n","        \n","        label = torch.tensor(self.df.label[index]).long()\n","        \n","        if self.transforms:\n","            augmentations = self.transforms(image=image)\n","            image = augmentations['image']\n","            \n","        return image, label"],"execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["train_dataset = dataset(train_df, transforms=train_transforms)\n","valid_dataset = dataset(valid_df, transforms=valid_transforms)\n","test_dataset = dataset(test_df, transforms=test_transforms)\n","\n","# check if datasets looks good\n","print(f'dataset len: {len(train_dataset)}, {len(valid_dataset)}')\n","print(f'image dtype: {train_dataset[44][0].dtype}, \\nlabel dtype: {train_dataset[44][1].dtype}')\n","print(f'dataset image shape: {train_dataset[44][0].shape}')\n","# train_dataset[44]"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["dataset len: 380, 60\n","image dtype: torch.float32, \n","label dtype: torch.int64\n","dataset image shape: torch.Size([3, 256, 256])\n"]}]},{"metadata":{},"cell_type":"markdown","source":["# dataloader"]},{"metadata":{"trusted":true},"cell_type":"code","source":["train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n","valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n","\n","print(f'train loader steps: {len(train_loader)}')"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["train loader steps: 12\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["# displayig the data\n","batch_tensor = next(iter(train_loader))[0][:10,...]\n","grid_img = torchvision.utils.make_grid(batch_tensor, nrow=5)\n","# grid_img.shape\n","plt.figure(figsize=(16,6))\n","# plt.imshow(grid_img.permute(1, 2, 0)); # uncomment to show"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<Figure size 1152x432 with 0 Axes>"]},"metadata":{},"execution_count":12},{"output_type":"display_data","data":{"text/plain":"<Figure size 1152x432 with 0 Axes>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["# check if dataloaders are ok\n","print(f'dataloader test: {next(iter(train_loader))[0].shape}, {next(iter(valid_loader))[0].shape}')"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["dataloader test: torch.Size([32, 3, 256, 256]), torch.Size([32, 3, 256, 256])\n"]}]},{"metadata":{},"cell_type":"markdown","source":["# load model, freeze layers"]},{"metadata":{"trusted":true},"cell_type":"code","source":["model = torchvision.models.resnext50_32x4d(pretrained=True)\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 2)\n","\n","model = model.to(device)"],"execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# criterion, optimizer and lr_scheduler"]},{"metadata":{"trusted":true},"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","\n","optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n","\n","lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)"],"execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# train model"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def train_model(n_epochs=1,\n","                model=model,\n","                train_loader=train_loader,\n","                valid_loader=valid_loader,\n","                criterion=criterion,\n","                optimizer=optimizer,\n","                lr_scheduler=lr_scheduler):\n","    \n","    total_time = time.time()\n","    print(f'================')\n","    print(f'started training...')\n","    \n","    for epoch in range(n_epochs):\n","        \n","        model.train()\n","        t0 = time.time()\n","        epoch_loss = 0\n","\n","        for batch, (images, labels) in enumerate(train_loader):\n","\n","            images = images.to(device, non_blocking=True)\n","            labels = labels.to(device, non_blocking=True)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","\n","            loss = criterion(outputs, labels)\n","            epoch_loss += loss\n","\n","            loss.backward()\n","            optimizer.step()\n","        \n","        train_avg_epoch_loss = round(float(epoch_loss/len(train_loader)), 4)\n","        valid_avg_epoch_loss, valid_epoch_accuracy = test_model(model, valid_loader)\n","        epoch_time = round(time.time() - t0)\n","        \n","        lr_scheduler.step()\n","        \n","        print(f'epoch: [{epoch+1}/{n_epochs}] | train loss: {train_avg_epoch_loss} | valid loss: {valid_avg_epoch_loss} | valid acc: {valid_epoch_accuracy} | time: {epoch_time//60:.0f}m {epoch_time%60:.0f}s')\n","    \n","    return model"],"execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# test model"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def test_model(model, valid_loader):\n","    \n","    model.eval()\n","    correct_on_epoch = 0\n","    total_num_images = 0\n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","        \n","        for batch, (images, labels) in enumerate(valid_loader):\n","            \n","            images = images.to(device, non_blocking=True)\n","            labels = labels.to(device, non_blocking=True)\n","            \n","            total_num_images += labels.size(0)\n","\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","            \n","            loss = criterion(outputs, labels)\n","            epoch_loss += loss\n","\n","            correct_on_epoch += (preds==labels).sum().item()\n","    \n","    valid_epoch_accuracy = round((correct_on_epoch/total_num_images), 4)\n","    valid_avg_epoch_loss = round(float(epoch_loss/len(valid_loader)), 4)\n","    \n","    return valid_avg_epoch_loss, valid_epoch_accuracy"],"execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# unfreeze"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def unfreeze(model=model):\n","    for param in model.parameters():\n","        param.requires_grad = True\n","    return model"],"execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# training model"]},{"metadata":{"trusted":true},"cell_type":"code","source":["start_time = time.time()\n","\n","train_model(frozen)\n","unfreeze()\n","train_model(unfrozen)\n","\n","total_time = time.time() - start_time\n","print(f'train time: {total_time//60:.0f}m {total_time%60:.0f}s')"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["================\n","started training...\n","epoch: [1/1] | train loss: 0.6887 | valid loss: 0.6021 | valid acc: 0.6833 | time: 2m 47s\n","================\n","started training...\n","epoch: [1/1] | train loss: 0.6087 | valid loss: 0.6506 | valid acc: 0.6 | time: 7m 28s\n","train time: 10m 15s\n"]}]},{"metadata":{},"cell_type":"markdown","source":["# saving model"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# checkpoint = {'model': model,\n","#               'state_dict': model.state_dict()}\n","\n","# torch.save(checkpoint, 'checkpoint_new.pth')"],"execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# test model no tta"]},{"metadata":{"trusted":true},"cell_type":"code","source":["_, test_acc = test_model(model, test_loader)\n","print(f'test set acc: {test_acc}')"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["test set acc: 0.6875\n"]}]},{"metadata":{},"cell_type":"markdown","source":["# tta"]},{"metadata":{"trusted":true},"cell_type":"code","source":["start_time = time.time()\n","\n","tta_crop = int(presize*0.9)\n","tta_model = tta.ClassificationTTAWrapper(model, tta.aliases.five_crop_transform(tta_crop, tta_crop))\n","\n","tta_transforms = A.Compose([\n","    A.SmallestMaxSize(presize),\n","    A.Normalize(),\n","    albumentations.pytorch.ToTensorV2()\n","])\n","\n","tta_dataset = dataset(test_df, transforms=tta_transforms)\n","tta_loader = DataLoader(tta_dataset, batch_size=1, shuffle=False, num_workers=0)\n","\n","_, tta_acc = test_model(tta_model, tta_loader)\n","\n","print(f'TTA acc: {tta_acc}')\n","\n","total_time = time.time() - start_time\n","print(f'TTA time: {total_time//60:.0f}m {total_time%60:.0f}s')"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["TTA acc: 0.6562\nTTA time: 2m 47s\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.9.2-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}