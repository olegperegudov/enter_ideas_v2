{"cells":[{"metadata":{},"cell_type":"markdown","source":"Project overview:\n\n- Dataset used: https://www.kaggle.com/gobsan/st-george-or-not\n- Dataset description: binary classification of ~6000 pics with St. George.\n\nThis NB attempts to classify a given dataset with Saint George using Pytorch and a pretrained resnet50 model. \nNo extra images were added to the dataset.\n\nThis is what was done:\n\n1. Prep work included using Pandas for path and label collection of all data.\n2. The data was split into train, valid and test datasets.\n3. Some minor augmentations added to the data.\n4. The model was 1st trained with all but the last layer frozen for a couple of epochs, then all params were unfrozen and the whole model trained again.\n5. tta was used on test dataset.\n\nThe model shows signs of overfitting the train set, but valid and, most importantly, test set error rates are good indicators of the model working properly."},{"metadata":{},"cell_type":"markdown","source":"# data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is needed for tta at the end\n!pip install ttach","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting ttach\n  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\nInstalling collected packages: ttach\nSuccessfully installed ttach-0.0.3\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imports\nimport torch\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport time\nfrom PIL import Image\nimport ttach as tta\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nimport albumentations as A\nimport albumentations.pytorch\nfrom sklearn.model_selection import train_test_split","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting seed\ntorch.manual_seed(0)\nnp.random.seed(0)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting device to cuda if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HYPERPARAMS HERE\n\n# params\nlr = 3e-3\nmomentum = 0.9\nweight_decay = 3e-3\n\n# transforms\npresize = 256\ncrop = 256\n\n# batch size\nbatch_size = 64\n\n# n_epochs\nfrozen = 3\nunfrozen = 6","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data preparation\npath = '../input/st-george-or-not/data'\npath_g = os.path.join(path, 'g')\npath_ng = os.path.join(path, 'n_g')\n\ndata_g = os.listdir(path_g)\ndata_ng = os.listdir(path_ng)\n\ndf_g = pd.DataFrame({'fnames': [os.path.join(path_g, i) for i in data_g], 'label':1})\ndf_ng = pd.DataFrame({'fnames': [os.path.join(path_ng, i) for i in data_ng], 'label':0})\n\ndf = pd.concat([df_g, df_ng], ignore_index=True)\n\nprint('df shape before re', df.shape)\ndf = df[df.fnames.str.contains('.*\\.jpg$')]\nprint('df shape after re', df.shape)\n\ndf = df.sample(frac=1).reset_index(drop=True)\n\n# df.head(10)","execution_count":6,"outputs":[{"output_type":"stream","text":"df shape before re (6049, 2)\ndf shape after re (5700, 2)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# train_test_split"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = df.iloc[:3700, :].reset_index(drop=True)\nvalid_df = df.iloc[3700:4700, :].reset_index(drop=True)\ntest_df = df.iloc[4700:, :].reset_index(drop=True)\n\ntrain_df.shape, valid_df.shape, test_df.shape","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"((3700, 2), (1000, 2), (1000, 2))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# transforms"},{"metadata":{"trusted":true},"cell_type":"code","source":"# albumentations transforms\n\ntrain_transforms = A.Compose([\n    A.SmallestMaxSize(presize),\n    A.RandomCrop(crop, crop),\n    A.Normalize(),\n    A.HorizontalFlip(),\n    A.Rotate(limit=30),\n    albumentations.pytorch.ToTensorV2()\n    ])\n\nvalid_transforms = A.Compose([\n    A.SmallestMaxSize(presize),\n    A.CenterCrop(crop, crop),\n    A.Normalize(),\n    A.HorizontalFlip(),\n    albumentations.pytorch.ToTensorV2()\n    ])\n\ntest_transforms = A.Compose([\n    A.SmallestMaxSize(presize),\n    A.CenterCrop(crop, crop),\n    A.Normalize(),\n    albumentations.pytorch.ToTensorV2()\n    ])","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# defining dataset class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class dataset(Dataset):\n    def __init__(self, df, transforms=None):\n        self.df=df\n        self.transforms=transforms\n        \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index):\n        image = Image.open(self.df.fnames[index]).convert('RGB')\n        image = np.array(image)\n        \n        label = torch.tensor(self.df.label[index]).long()\n        \n        if self.transforms:\n            augmentations = self.transforms(image=image)\n            image = augmentations['image']\n            \n        return image, label","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = dataset(train_df, transforms=train_transforms)\nvalid_dataset = dataset(valid_df, transforms=valid_transforms)\ntest_dataset = dataset(test_df, transforms=test_transforms)\n\n# check if datasets looks good\nprint(f'dataset len: {len(train_dataset)}, {len(valid_dataset)}')\nprint(f'image dtype: {train_dataset[44][0].dtype}, \\nlabel dtype: {train_dataset[44][1].dtype}')\nprint(f'dataset image shape: {train_dataset[44][0].shape}')\n# train_dataset[44]","execution_count":10,"outputs":[{"output_type":"stream","text":"dataset len: 3700, 1000\nimage dtype: torch.float32, \nlabel dtype: torch.int64\ndataset image shape: torch.Size([3, 256, 256])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n\nprint(f'train loader steps: {len(train_loader)}')","execution_count":11,"outputs":[{"output_type":"stream","text":"train loader steps: 58\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# displayig the data\nbatch_tensor = next(iter(train_loader))[0][:10,...]\ngrid_img = torchvision.utils.make_grid(batch_tensor, nrow=5)\n# grid_img.shape\nplt.figure(figsize=(16,6))\n# plt.imshow(grid_img.permute(1, 2, 0)); # uncomment to show","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"<Figure size 1152x432 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1152x432 with 0 Axes>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if dataloaders are ok\nprint(f'dataloader test: {next(iter(train_loader))[0].shape}, {next(iter(valid_loader))[0].shape}')","execution_count":13,"outputs":[{"output_type":"stream","text":"dataloader test: torch.Size([64, 3, 256, 256]), torch.Size([64, 3, 256, 256])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# load model, freeze layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = torchvision.models.resnext50_32x4d(pretrained=True)\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 2)\n\nmodel = model.to(device)","execution_count":14,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-7cdf4587.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/95.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dc67a942fc94f37b5931c648927c218"}},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# criterion, optimizer and lr_scheduler"},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\noptimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n\nlr_scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(n_epochs=1,\n                model=model,\n                train_loader=train_loader,\n                valid_loader=valid_loader,\n                criterion=criterion,\n                optimizer=optimizer,\n                lr_scheduler=lr_scheduler):\n    \n    total_time = time.time()\n    print(f'================')\n    print(f'started training...')\n    \n    for epoch in range(n_epochs):\n        \n        model.train()\n        t0 = time.time()\n        epoch_loss = 0\n\n        for batch, (images, labels) in enumerate(train_loader):\n\n            images = images.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n\n            loss = criterion(outputs, labels)\n            epoch_loss += loss\n\n            loss.backward()\n            optimizer.step()\n        \n        train_avg_epoch_loss = round(float(epoch_loss/len(train_loader)), 4)\n        valid_avg_epoch_loss, valid_epoch_accuracy = test_model(model, valid_loader)\n        epoch_time = round(time.time() - t0)\n        \n        lr_scheduler.step()\n        \n        print(f'epoch: [{epoch+1}/{n_epochs}] | train loss: {train_avg_epoch_loss} | valid loss: {valid_avg_epoch_loss} | valid acc: {valid_epoch_accuracy} | time: {epoch_time//60:.0f}m {epoch_time%60:.0f}s')\n    \n    return model","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# test model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_model(model, valid_loader):\n    \n    model.eval()\n    correct_on_epoch = 0\n    total_num_images = 0\n    epoch_loss = 0\n    \n    with torch.no_grad():\n        \n        for batch, (images, labels) in enumerate(valid_loader):\n            \n            images = images.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n            \n            total_num_images += labels.size(0)\n\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            \n            loss = criterion(outputs, labels)\n            epoch_loss += loss\n\n            correct_on_epoch += (preds==labels).sum().item()\n    \n    valid_epoch_accuracy = round((correct_on_epoch/total_num_images), 4)\n    valid_avg_epoch_loss = round(float(epoch_loss/len(valid_loader)), 4)\n    \n    return valid_avg_epoch_loss, valid_epoch_accuracy","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# unfreeze"},{"metadata":{"trusted":true},"cell_type":"code","source":"def unfreeze(model=model):\n    for param in model.parameters():\n        param.requires_grad = True\n    return model","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# training model"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\n\ntrain_model(frozen)\nunfreeze()\ntrain_model(unfrozen)\n\ntotal_time = time.time() - start_time\nprint(f'train time: {total_time//60:.0f}m {total_time%60:.0f}s')","execution_count":19,"outputs":[{"output_type":"stream","text":"================\nstarted training...\nepoch: [1/3] | train loss: 0.5415 | valid loss: 0.4331 | valid acc: 0.803 | time: 2m 11s\nepoch: [2/3] | train loss: 0.4283 | valid loss: 0.4103 | valid acc: 0.801 | time: 1m 29s\nepoch: [3/3] | train loss: 0.3974 | valid loss: 0.3782 | valid acc: 0.832 | time: 1m 28s\n================\nstarted training...\nepoch: [1/6] | train loss: 0.3355 | valid loss: 0.5323 | valid acc: 0.811 | time: 1m 45s\nepoch: [2/6] | train loss: 0.1529 | valid loss: 0.2328 | valid acc: 0.914 | time: 1m 45s\nepoch: [3/6] | train loss: 0.1125 | valid loss: 0.234 | valid acc: 0.917 | time: 1m 45s\nepoch: [4/6] | train loss: 0.0984 | valid loss: 0.2349 | valid acc: 0.914 | time: 1m 45s\nepoch: [5/6] | train loss: 0.0892 | valid loss: 0.2252 | valid acc: 0.918 | time: 1m 46s\nepoch: [6/6] | train loss: 0.0871 | valid loss: 0.2249 | valid acc: 0.914 | time: 1m 44s\ntrain time: 15m 38s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# saving model"},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = {'model': model,\n              'state_dict': model.state_dict()}\n\ntorch.save(checkpoint, 'checkpoint.pth')","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# test model no tta"},{"metadata":{"trusted":true},"cell_type":"code","source":"_, test_acc = test_model(model, test_loader)\nprint(f'test set acc: {test_acc}')","execution_count":21,"outputs":[{"output_type":"stream","text":"test set acc: 0.927\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# tta"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\n\ntta_crop = int(presize*0.9)\ntta_model = tta.ClassificationTTAWrapper(model, tta.aliases.five_crop_transform(tta_crop, tta_crop))\n\ntta_transforms = A.Compose([\n    A.SmallestMaxSize(presize),\n    A.Normalize(),\n    albumentations.pytorch.ToTensorV2()\n])\n\ntta_dataset = dataset(test_df, transforms=tta_transforms)\ntta_loader = DataLoader(tta_dataset, batch_size=1, shuffle=False, num_workers=1)\n\n_, tta_acc = test_model(tta_model, tta_loader)\n\nprint(f'TTA acc: {tta_acc}')\n\ntotal_time = time.time() - start_time\nprint(f'TTA time: {total_time//60:.0f}m {total_time%60:.0f}s')","execution_count":22,"outputs":[{"output_type":"stream","text":"TTA acc: 0.929\nTTA time: 1m 47s\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}